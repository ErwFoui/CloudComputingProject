{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stream Consumption & Writing to Cassandra "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Cassandra workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4040 --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0,com.datastax.spark:spark-cassandra-connector_2.11:2.0.0-M3 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"CassandraWriter\") \\\n",
    "    .setMaster(\"local[2]\") \\\n",
    "    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "    \n",
    "spark = SparkContext(conf=conf)\n",
    "sqlContext=SQLContext(spark)\n",
    "\n",
    "data_path = \"../Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "import functools\n",
    "\n",
    "\n",
    "# Define helper function to concatenate streaming DataFrames\n",
    "def unionAll(df_list):\n",
    "    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), df_list)\n",
    "\n",
    "\n",
    "dfs = []\n",
    "year = 2008\n",
    "for month in range(1, 13):\n",
    "    csv_path = data_path + \"/\" + str(year) + \"/\" + str(month)\n",
    "\n",
    "#         Use of Spark static DataFrames because aggregation operations (e.g., JOINS) are not supported for streaming datasets \n",
    "    dfs.append(sqlContext.read.options(header='True', inferSchema='True', delimiter=',').csv(data_path + \"/\" + str(year) + \"/\" + str(month) + \"/\" + str(month) + \".csv\"))\n",
    "        \n",
    "df = unionAll(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"flights_2008\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) To save data to Cassandra, please run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Cassandra requires a unique primary key, that we add here + column names must be in lowercase format\n",
    "# from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "\n",
    "# df_index = df.toDF(*[col.lower() for col in df.columns]).withColumn(\"id\", monotonically_increasing_id())\n",
    "# df_index.createOrReplaceTempView(\"flights_2008\")\n",
    "# df_index.write.format(\"org.apache.spark.sql.cassandra\").mode('overwrite').options(table=\"flights_2008\", keyspace=\"task2\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 \n",
    "\n",
    "Tom wants to travel from airport X to airport Z. However, Tom also wants to stop at airport Y for some sightseeing on the way. More concretely, Tom has the following requirements (for specific queries, see the Task 1 Queries and Task 2 Queries):\n",
    "\n",
    "a) The second leg of the journey (flight Y-Z) must depart two days after the first leg (flight X-Y). For example, if X-Y departs on January 5, 2008, Y-Z must depart on January 7, 2008.\n",
    "\n",
    "b) Tom wants his flights scheduled to depart airport X before 12:00 PM local time and to depart airport Y after 12:00 PM local time.\n",
    "\n",
    "c) Tom wants to arrive at each destination with as little delay as possible. You can assume you know the actual delay of each flight.\n",
    "\n",
    "Your mission (should you choose to accept it!) is to find, for each X-Y-Z and day/month (dd/mm) combination in the year 2008, the two flights (X-Y and Y-Z) that satisfy constraints (a) and (b) and have the best individual performance with respect to constraint (c), if such flights exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First leg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_3_2_xy = sqlContext.sql(\"SELECT\\\n",
    "                                  ORIGIN,\\\n",
    "                                  FL_DATE,\\\n",
    "                                  CRS_DEP_TIME,\\\n",
    "                                  OP_UNIQUE_CARRIER,\\\n",
    "                                  OP_CARRIER_FL_NUM,\\\n",
    "                                  DEST,\\\n",
    "                                  ARR_DELAY,\\\n",
    "                                  rank() OVER (PARTITION BY ORIGIN, DEST, YEAR, MONTH ORDER BY ARR_DELAY ASC) AS rank\\\n",
    "                              FROM flights_2008\\\n",
    "                              WHERE CRS_DEP_TIME < 1200 AND CANCELLED = 0\")\n",
    "\n",
    "table_3_2_xy.createOrReplaceTempView(\"xy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+-----------------+-----------------+----+---------+----+\n",
      "|ORIGIN|FL_DATE|CRS_DEP_TIME|OP_UNIQUE_CARRIER|OP_CARRIER_FL_NUM|DEST|ARR_DELAY|rank|\n",
      "+------+-------+------------+-----------------+-----------------+----+---------+----+\n",
      "|   ABE|8/19/08|         630|               EV|             4171| ATL|      -18|   1|\n",
      "|   ABE| 8/5/08|         630|               EV|             4171| ATL|      -17|   2|\n",
      "|   ABE| 8/1/08|         630|               EV|             4171| ATL|      -14|   3|\n",
      "|   ABE| 8/9/08|         630|               EV|             4171| ATL|      -14|   3|\n",
      "|   ABE|8/11/08|         630|               EV|             4171| ATL|      -14|   3|\n",
      "|   ABE|8/29/08|         630|               EV|             4171| ATL|      -14|   3|\n",
      "|   ABE| 8/3/08|         630|               EV|             4171| ATL|      -13|   7|\n",
      "|   ABE| 8/6/08|         630|               EV|             4171| ATL|      -12|   8|\n",
      "|   ABE|8/31/08|         630|               EV|             4171| ATL|      -10|   9|\n",
      "|   ABE|8/23/08|         630|               EV|             4171| ATL|       -9|  10|\n",
      "|   ABE|8/10/08|         630|               EV|             4171| ATL|       -7|  11|\n",
      "|   ABE| 8/4/08|         630|               EV|             4171| ATL|       -6|  12|\n",
      "|   ABE|8/21/08|         630|               EV|             4171| ATL|       -3|  13|\n",
      "|   ABE| 8/2/08|         630|               EV|             4171| ATL|       -2|  14|\n",
      "|   ABE| 8/7/08|         630|               EV|             4171| ATL|       -2|  14|\n",
      "|   ABE|8/20/08|         630|               EV|             4171| ATL|       -2|  14|\n",
      "|   ABE|8/25/08|         630|               EV|             4171| ATL|        0|  17|\n",
      "|   ABE|8/18/08|         630|               EV|             4171| ATL|        1|  18|\n",
      "|   ABE|8/22/08|         630|               EV|             4171| ATL|        4|  19|\n",
      "|   ABE|8/30/08|         630|               EV|             4171| ATL|        4|  19|\n",
      "+------+-------+------------+-----------------+-----------------+----+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_3_2_xy.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second leg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_3_2_yz = sqlContext.sql(\"SELECT\\\n",
    "                                  ORIGIN,\\\n",
    "                                  FL_DATE,\\\n",
    "                                  CRS_DEP_TIME,\\\n",
    "                                  OP_UNIQUE_CARRIER,\\\n",
    "                                  OP_CARRIER_FL_NUM,\\\n",
    "                                  DEST,\\\n",
    "                                  ARR_DELAY,\\\n",
    "                                  rank() OVER (PARTITION BY ORIGIN, DEST, YEAR, MONTH ORDER BY ARR_DELAY ASC) AS rank\\\n",
    "                              FROM flights_2008\\\n",
    "                              WHERE CRS_DEP_TIME > 1200 AND CANCELLED = 0\")\n",
    "\n",
    "table_3_2_yz.createOrReplaceTempView(\"yz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+-----------------+-----------------+----+---------+----+\n",
      "|ORIGIN|FL_DATE|CRS_DEP_TIME|OP_UNIQUE_CARRIER|OP_CARRIER_FL_NUM|DEST|ARR_DELAY|rank|\n",
      "+------+-------+------------+-----------------+-----------------+----+---------+----+\n",
      "|   ABE| 8/9/08|        1545|               EV|             4598| ATL|      -24|   1|\n",
      "|   ABE|8/13/08|        1545|               EV|             4598| ATL|      -12|   2|\n",
      "|   ABE|8/16/08|        1545|               EV|             4598| ATL|      -11|   3|\n",
      "|   ABE|8/21/08|        1545|               EV|             4598| ATL|      -11|   3|\n",
      "|   ABE|8/23/08|        1545|               EV|             4598| ATL|      -10|   5|\n",
      "|   ABE| 8/6/08|        1545|               EV|             4598| ATL|       -9|   6|\n",
      "|   ABE|8/11/08|        1545|               EV|             4598| ATL|       -8|   7|\n",
      "|   ABE|8/19/08|        1545|               EV|             4598| ATL|       -7|   8|\n",
      "|   ABE|8/29/08|        1545|               EV|             4598| ATL|       -7|   8|\n",
      "|   ABE|8/17/08|        1545|               EV|             4598| ATL|       -1|  10|\n",
      "|   ABE|8/27/08|        1545|               EV|             4598| ATL|        1|  11|\n",
      "|   ABE| 8/3/08|        1545|               EV|             4598| ATL|        2|  12|\n",
      "|   ABE| 8/4/08|        1545|               EV|             4598| ATL|        2|  12|\n",
      "|   ABE|8/30/08|        1545|               EV|             4598| ATL|        2|  12|\n",
      "|   ABE|8/18/08|        1545|               EV|             4598| ATL|        3|  15|\n",
      "|   ABE|8/10/08|        1545|               EV|             4598| ATL|        4|  16|\n",
      "|   ABE|8/24/08|        1545|               EV|             4598| ATL|        6|  17|\n",
      "|   ABE|8/31/08|        1545|               EV|             4598| ATL|        7|  18|\n",
      "|   ABE| 8/2/08|        1545|               EV|             4598| ATL|        8|  19|\n",
      "|   ABE|8/12/08|        1545|               EV|             4598| ATL|        9|  20|\n",
      "+------+-------+------------+-----------------+-----------------+----+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_3_2_yz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join legs and register to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "table_3_2 = sqlContext.sql(\"SELECT\\\n",
    "                              xy.ORIGIN AS x,\\\n",
    "                              xy.DEST AS y,\\\n",
    "                              yz.DEST AS z,\\\n",
    "                              xy.FL_DATE AS xy_fl_date,\\\n",
    "                              xy.CRS_DEP_TIME AS xy_dep_time,\\\n",
    "                              xy.OP_UNIQUE_CARRIER AS carrier_xy,\\\n",
    "                              xy.OP_CARRIER_FL_NUM AS flight_xy,\\\n",
    "                              yz.FL_DATE AS yz_fl_date,\\\n",
    "                              yz.CRS_DEP_TIME AS yz_dep_time,\\\n",
    "                              yz.OP_UNIQUE_CARRIER AS carrier_yz,\\\n",
    "                              yz.OP_CARRIER_FL_NUM AS flight_yz,\\\n",
    "                              xy.ARR_DELAY + yz.ARR_DELAY AS total_delay\\\n",
    "                          FROM xy JOIN yz ON\\\n",
    "                               xy.rank = 1 AND yz.rank = 1 AND xy.DEST = yz.ORIGIN AND date_add(xy.FL_DATE, 2) = yz.FL_DATE\")\n",
    "\n",
    "table_3_2.createOrReplaceTempView(\"flights_3_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) To save data to Cassandra, please run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# table_3_2.write.format(\"org.apache.spark.sql.cassandra\").mode('overwrite').options(table=\"flights_3_2\", keyspace=\"task2\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(Optional) To extract data from Cassandra, please run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query_3_2 = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"flights_3_2\", keyspace=\"task2\").load()\n",
    "# query_3_2.createOrReplaceTempView(\"flights_3_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----------+----------+---------+----------+----------+---------+-----------+\n",
      "|  x|  y|  z|xy_fl_date|carrier_xy|flight_xy|yz_fl_date|carrier_yz|flight_yz|total_delay|\n",
      "+---+---+---+----------+----------+---------+----------+----------+---------+-----------+\n",
      "|BOS|ATL|LAX|    4/3/08|        FL|      270|    4/5/08|        FL|       40|          5|\n",
      "|PHX|JFK|MSP|    9/7/08|        B6|      178|    9/9/08|        NW|      609|        -42|\n",
      "|DFW|STL|ORD|   1/14/08|        AA|     1336|   1/16/08|        AA|     2245|        -19|\n",
      "|LAX|MIA|LAX|   5/16/08|        AA|      280|   5/18/08|        AA|      456|         -9|\n",
      "+---+---+---+----------+----------+---------+----------+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT x, y, z, xy_fl_date, carrier_xy, flight_xy, yz_fl_date, carrier_yz, flight_yz, total_delay\\\n",
    "                FROM flights_3_2 \\\n",
    "                WHERE (x='BOS' AND y='ATL' AND z='LAX' AND xy_FL_DATE='4/3/08')\\\n",
    "                OR (x='PHX' AND y='JFK' AND z='MSP' AND xy_FL_DATE='9/7/08')\\\n",
    "                OR (x='DFW' AND y='STL' AND z='ORD' AND xy_FL_DATE='1/14/08')\\\n",
    "                OR (x='LAX' AND y='MIA' AND z='LAX' AND xy_FL_DATE='5/16/08')\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
