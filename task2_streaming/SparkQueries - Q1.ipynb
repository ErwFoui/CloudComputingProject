{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4040 --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0,com.datastax.spark:spark-cassandra-connector_2.11:2.0.0-M3 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Define Spark Datasets from CSVs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"csv file reader\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data_path = \"../Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "import functools\n",
    "\n",
    "\n",
    "# Define helper function to concatenate streaming DataFrames\n",
    "def unionAll(df_list):\n",
    "    return functools.reduce(lambda df1,df2: df1.union(df2.select(df1.columns)), df_list) \n",
    "\n",
    "\n",
    "dfs = []\n",
    "for year in range(1995, 2009):\n",
    "# for year in range(2000, 2009):\n",
    "    print(year)\n",
    "    for month in range(1, 13):\n",
    "#     for month in range(1, 3):\n",
    "        csv_path = data_path + \"/\" + str(year) + \"/\" + str(month)\n",
    "#         Use of Spark static DataFrames because aggregation operations (e.g., JOINS) are not supported for streaming datasets \n",
    "#         dfs.append(spark.readStream.option(\"sep\", \",\").option(\"header\", \"True\").schema(userSchema).csv(csv_path))\n",
    "        \n",
    "        dfs.append(spark.read.options(header='True', inferSchema='True', delimiter=',').csv(data_path + \"/\" + str(year) + \"/\" + str(month) + \"/\" + str(month) + \".csv\"))\n",
    "        \n",
    "df = unionAll(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[YEAR: int, MONTH: int, DAY_OF_MONTH: int, DAY_OF_WEEK: int, FL_DATE: string, OP_UNIQUE_CARRIER: string, OP_CARRIER_FL_NUM: int, ORIGIN_AIRPORT_ID: int, ORIGIN: string, DEST_AIRPORT_ID: int, DEST: string, CRS_DEP_TIME: int, DEP_TIME: int, DEP_DELAY: int, DEP_DELAY_NEW: int, CRS_ARR_TIME: int, ARR_TIME: int, ARR_DELAY: int, ARR_DELAY_NEW: int, CANCELLED: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Q1 - Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Rank the top 10 most popular airports by numbers of flights to/from the airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "origin = spark.sql(\"SELECT ORIGIN, count(OP_CARRIER_FL_NUM) AS all_departures FROM flights GROUP BY ORIGIN\")\n",
    "destination = spark.sql(\"SELECT DEST, count(OP_CARRIER_FL_NUM) AS all_arrivals FROM flights GROUP BY DEST\")\n",
    "\n",
    "joined_df = (origin\n",
    "             .join(destination, origin.ORIGIN == destination.DEST)\n",
    "             .withColumn(\"all_flights\", col(\"all_departures\") + col(\"all_arrivals\"))\n",
    "             .select(\"ORIGIN\", \"all_flights\")\n",
    "            )\n",
    "\n",
    "joined_df.createOrReplaceTempView(\"Q1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|ORIGIN|all_flights|\n",
      "+------+-----------+\n",
      "|   ORD|    9252980|\n",
      "|   ATL|    8885867|\n",
      "|   DFW|    7926248|\n",
      "|   LAX|    5844795|\n",
      "|   PHX|    5047005|\n",
      "|   DEN|    4508601|\n",
      "|   IAH|    4423809|\n",
      "|   DTW|    4197592|\n",
      "|   LAS|    4024215|\n",
      "|   MSP|    3939329|\n",
      "+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_1_1 = spark.sql(\"SELECT * FROM Q1 GROUP BY ORIGIN, all_flights ORDER BY all_flights DESC\")\n",
    "query_1_1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Rank the top 10 airlines by on-time arrival performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|airline|avg_arr_delay|\n",
      "+-------+-------------+\n",
      "|     HA|       -0.775|\n",
      "|     KH|        1.157|\n",
      "|     F9|        5.693|\n",
      "|     WN|         5.84|\n",
      "|     OO|        5.877|\n",
      "|     9E|        6.108|\n",
      "|     TZ|        6.129|\n",
      "|     NW|        6.298|\n",
      "|     US|        6.471|\n",
      "|     DH|        6.798|\n",
      "+-------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_1_2 = spark.sql(\"SELECT\\\n",
    "                          OP_UNIQUE_CARRIER AS airline,\\\n",
    "                          round(avg(ARR_DELAY), 3) AS avg_arr_delay\\\n",
    "                      FROM flights\\\n",
    "                      WHERE CANCELLED=0 AND ARR_DELAY IS NOT NULL\\\n",
    "                      GROUP BY OP_UNIQUE_CARRIER\\\n",
    "                      ORDER BY avg_arr_delay ASC\")\n",
    "query_1_2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Rank the days of the week by on-time arrival performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "|dow|avg_arr_delay|\n",
      "+---+-------------+\n",
      "|  6|        4.281|\n",
      "|  2|        6.008|\n",
      "|  3|        7.172|\n",
      "|  7|        7.249|\n",
      "|  1|        7.297|\n",
      "|  4|        9.341|\n",
      "|  5|       10.257|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_1_3 = spark.sql(\"SELECT\\\n",
    "                          DAY_OF_WEEK AS dow,\\\n",
    "                          round(avg(ARR_DELAY), 3) AS avg_arr_delay\\\n",
    "                      FROM flights\\\n",
    "                      WHERE CANCELLED=0 AND ARR_DELAY IS NOT NULL\\\n",
    "                      GROUP BY DAY_OF_WEEK\\\n",
    "                      ORDER BY avg_arr_delay ASC\")\n",
    "query_1_3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
